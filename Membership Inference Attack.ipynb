{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJ33E-3YrVD-"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHwBjWkKqs0r"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy import ndimage\n",
    "from IPython.display import HTML\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE5VdLYVFZgr"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "targetclass = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85EQeiNzl4fD"
   },
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    trans = np.transpose(npimg, (1,2,0))\n",
    "    return np.squeeze(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5fXJSmz1Qs1"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Fg9SwPurYwu"
   },
   "source": [
    "# Data Entry and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxwbMjoErc73"
   },
   "outputs": [],
   "source": [
    "# Transform image to tensor and normalize features from [0,255] to [0,1]\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,),(0.5,),(0.5)),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "4c3251b4e7144cd3a5a92c2d8c00854a",
      "01824765ed9e4234adca0786222543ff",
      "0024b395aaec4983a874c9c3f92af84e",
      "7d3fb8e3c37e46ebaaa1967e787e6a4e",
      "d901d93b5f09484ea6c8ea6dfb9d8bcc",
      "0c20e162490c433e83a3b82ac973a826",
      "aede667a8fa3464080eb6e029b271bcf",
      "64d5070ef1d149a3af18a3b1fa0e3989"
     ]
    },
    "colab_type": "code",
    "id": "MXC2v1cDsR6o",
    "outputId": "26dd3357-e550-43f0-e44a-7f6c937f06f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Using CIFAR100\n",
    "traindata = datasets.CIFAR100('/data', download=True, train=True, transform=transform)\n",
    "testdata = datasets.CIFAR100('/data', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train loaders containing the sensitive data class\n",
    "# and the non-sensitive data\n",
    "target_index = []\n",
    "nontarget_index = []\n",
    "for i in range(0, len(in_data)):\n",
    "  if in_data[i][1] == targetclass:\n",
    "    target_index.append(i)\n",
    "  else:\n",
    "    nontarget_index.append(i)\n",
    "# target_train_loader is a dataloader for the sensitive data that\n",
    "# we are targeting for removal\n",
    "target_train_loader = torch.utils.data.DataLoader(in_data, batch_size=64,\n",
    "              sampler = torch.utils.data.SubsetRandomSampler(target_index))\n",
    "# nontarget_train_loader contains all other data\n",
    "nontarget_train_loader = torch.utils.data.DataLoader(in_data, batch_size=64,\n",
    "              sampler = torch.utils.data.SubsetRandomSampler(nontarget_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the unlearning data removal method, we randomly\n",
    "# relabel all sensitive examples\n",
    "unlearningdata = copy.deepcopy(traindata)\n",
    "unlearninglabels = list(range(100))\n",
    "unlearninglabels.remove(targetclass)\n",
    "for data in unlearningdata:\n",
    "  if unlearningdata.targets == targetclass:\n",
    "    unlearningdata.targets = random.choice(unlearninglabels)\n",
    "unlearning_train_loader = torch.utils.data.DataLoader(unlearningdata, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GHiRlkzwKnB"
   },
   "source": [
    "# Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-YvXNaj2Olp"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "torch.backends.cudnn.enabled = True\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r7pQAbYyFl7"
   },
   "outputs": [],
   "source": [
    "# Training method\n",
    "def train(model, optimizer, epoch, loader, printable=True):\n",
    "  model.train()\n",
    "  batches = []\n",
    "  steps = []\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0 and printable:\n",
    "      print(\"Epoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n",
    "          epoch, batch_idx*len(data),  loss.item()\n",
    "      ))\n",
    "  return batches, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method that returns recall and miss rates during training\n",
    "def train2(model, optimizer, epoch, loader, printable=True):\n",
    "  model.train()\n",
    "  recall = []\n",
    "  missrate = []\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  r, m = testtargetmodel()\n",
    "  recall.append(r)\n",
    "  missrate.append(m)\n",
    "  return recall, missrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method that keeps a list of parameter updates from\n",
    "# batches containing sensitive data for amnesiac unlearning\n",
    "def selectivetrain(model, optimizer, epoch, loader, returnable=False):\n",
    "  model.train()\n",
    "  delta = {}\n",
    "  for param_tensor in model.state_dict():\n",
    "        if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "            delta[param_tensor] = 0\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    if targetclass in target:\n",
    "      before = {}\n",
    "      for param_tensor in model.state_dict():\n",
    "        if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "          before[param_tensor] = model.state_dict()[param_tensor].clone()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if targetclass in target:\n",
    "      after = {}\n",
    "      for param_tensor in model.state_dict():\n",
    "        if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "          after[param_tensor] = model.state_dict()[param_tensor].clone()\n",
    "      for key in before:\n",
    "        delta[key] = delta[key] + after[key] - before[key]\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print(\"\\rEpoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n",
    "          epoch, batch_idx*len(data),  loss.item()\n",
    "      ), end=\"\")\n",
    "  if returnable:\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y10g3tLd0zTs"
   },
   "outputs": [],
   "source": [
    "# Testing method\n",
    "def test(model, loader, dname=\"Test set\", printable=True):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      total += target.size()[0]\n",
    "      test_loss += criterion(output, target).item()\n",
    "      _, pred = torch.topk(output, 10, dim=1, largest=True, sorted=True)\n",
    "      for i, t in enumerate(target):\n",
    "        if t in pred[i]:\n",
    "            correct += 1\n",
    "  test_loss /= len(loader.dataset)\n",
    "  if printable:\n",
    "    print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        dname, test_loss, correct, total, \n",
    "        100. * correct / total\n",
    "        ))\n",
    "  return 1. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    # load resnet 18 and change to fit problem dimensionality\n",
    "    resnet = models.resnet18()\n",
    "    resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "    resnet.fc = nn.Sequential(nn.Linear(512, 100))\n",
    "    optimizer = optim.Adam(resnet.parameters())\n",
    "    return resnet, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN attack model for membership inference attack\n",
    "class AttackModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AttackModel, self).__init__()\n",
    "    self.fc1 = nn.Linear(100, 256)\n",
    "    self.fc2 = nn.Linear(256, 128)\n",
    "    self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc3(x)\n",
    "    return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate attack models\n",
    "def attack_model_fn():\n",
    "    \n",
    "  model = AttackModel()\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "  return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method for attack model\n",
    "def trainattacker(model, optimizer, epoch, loader, printable=True):\n",
    "  model.train()\n",
    "  batches = []\n",
    "  steps = []\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    output = torch.flatten(output)\n",
    "    loss = F.binary_cross_entropy(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0 and printable:\n",
    "      print(\"\\rEpoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n",
    "          epoch, batch_idx*len(data),  loss.item()/len(loader.dataset)\n",
    "      ), end=\"\")\n",
    "  return batches, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing method for attack model\n",
    "def testattacker(model, loader, dname=\"Test set\", printable=True):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      output = torch.flatten(output)\n",
    "      total += target.size()[0]\n",
    "      test_loss += F.binary_cross_entropy(output, target).item()\n",
    "      pred = torch.round(output)\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(loader.dataset)\n",
    "  if printable:\n",
    "    print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        dname, test_loss, correct, total, \n",
    "        100. * correct / total\n",
    "        ))\n",
    "  return 1. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing method for attack that returns full confusion matrix\n",
    "def fulltestattacker(model, loader, dname=\"Test set\", printable=True):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      output = torch.flatten(output)\n",
    "      pred = torch.round(output)\n",
    "#       correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "      for i in range(len(pred)):\n",
    "        if pred[i] == target[i] == 1:\n",
    "            tp += 1\n",
    "        if pred[i] == target[i] == 0:\n",
    "            tn += 1\n",
    "        if pred[i] == 1 and target[i] == 0:\n",
    "            fp += 1\n",
    "        if pred[i] == 0 and target[i] == 1:\n",
    "            fn += 1\n",
    "  return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYqbV-DjPSTV"
   },
   "source": [
    "# Training Shadow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2bbj_QKPUH6"
   },
   "outputs": [],
   "source": [
    "num_shadow_models = 20\n",
    "shadow_training_epochs = 10\n",
    "log_interval = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shadow models\n",
    "shadow_models = []\n",
    "for _ in range(num_shadow_models):\n",
    "  shadow_models.append(target_model_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shadow datasets. Each must have an \"in\" and \"out\" set for attack model\n",
    "# dataset generation ([in, out]). Each shadow model is trained only on the \"in\"\n",
    "# data.\n",
    "shadow_datasets = []\n",
    "for i in range(num_shadow_models):\n",
    "  shadow_datasets.append(torch.utils.data.random_split(traindata, [int(len(traindata)/2), int(len(traindata)/2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch can save any serialized object, which is very\n",
    "# helpful in this instance\n",
    "path = f\"infattack/resnet_datasets.pt\"\n",
    "torch.save(shadow_datasets, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shadow model 0\n",
      "\tEpoch 10  All data: Mean loss: 0.4166, Accuracy: 7452/10000 (75%)\n",
      "\tTime taken: 9696.398878836999\n",
      "Training shadow model 1\n",
      "\tEpoch 10  All data: Mean loss: 0.4665, Accuracy: 7376/10000 (74%)\n",
      "\tTime taken: 9699.79302728\n",
      "Training shadow model 2\n",
      "\tEpoch 10  All data: Mean loss: 0.4835, Accuracy: 7236/10000 (72%)\n",
      "\tTime taken: 9710.195670124001\n",
      "Training shadow model 3\n",
      "\tEpoch 10  All data: Mean loss: 0.3711, Accuracy: 7435/10000 (74%)\n",
      "\tTime taken: 9711.715010443\n",
      "Training shadow model 4\n",
      "\tEpoch 6  "
     ]
    }
   ],
   "source": [
    "# We need to train each shadow model on the in_data for that model\n",
    "for i, shadow_model_set in enumerate(shadow_models):\n",
    "  starttime = time.process_time()\n",
    "  shadow_model = shadow_model_set[0]\n",
    "  shadow_optim = shadow_model_set[1]\n",
    "  in_loader = torch.utils.data.DataLoader(shadow_datasets[i][0], batch_size=batch_size, shuffle=True)\n",
    "  print(f\"Training shadow model {i}\")\n",
    "  for epoch in range(1, shadow_training_epochs+1):\n",
    "    print(f\"\\r\\tEpoch {epoch}  \"  , end=\"\")\n",
    "    train(shadow_model, shadow_optim, epoch, in_loader, printable=False)\n",
    "    if epoch == shadow_training_epochs:\n",
    "      test(shadow_model, testloader, dname=\"All data\", printable=True)\n",
    "  path = F\"infattack/resnet-shadow_model_{i}.pt\"\n",
    "  torch.save({\n",
    "            'model_state_dict': shadow_model.state_dict(),\n",
    "            }, path)\n",
    "  print(f\"\\tTime taken: {time.process_time() - starttime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMoryz1taRIt"
   },
   "source": [
    "# Generating Attack Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "97YJeDb3aPFy",
    "outputId": "60196c3d-7bfa-4cd1-baf3-5c6c7e33628f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating class 0 set from model 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating class 0 set from model 19tensor([5004, 3082])\n",
      "(281, 273, 233, 22)\n",
      "Time taken: 525.9881777319824\n",
      "Generating class 1 set from model 19tensor([4960, 2815])\n",
      "(256, 332, 171, 19)\n",
      "Time taken: 515.6521431499859\n",
      "Generating class 2 set from model 19tensor([5005, 1701])\n",
      "(146, 431, 69, 25)\n",
      "Time taken: 498.80889649601886\n",
      "Generating class 3 set from model 19tensor([4949, 1362])\n",
      "(115, 443, 48, 26)\n",
      "Time taken: 487.1815901249938\n",
      "Generating class 4 set from model 19tensor([5014, 1481])\n",
      "(114, 438, 72, 26)\n",
      "Time taken: 495.0305571350036\n",
      "Generating class 5 set from model 19tensor([4977, 3134])\n",
      "(283, 337, 162, 30)\n",
      "Time taken: 521.4092788709968\n",
      "Generating class 6 set from model 19tensor([5098, 2084])\n",
      "(193, 411, 108, 7)\n",
      "Time taken: 502.26963512500515\n",
      "Generating class 7 set from model 19tensor([5007, 2244])\n",
      "(205, 363, 122, 36)\n",
      "Time taken: 505.331816239981\n",
      "Generating class 8 set from model 19tensor([5011, 2920])\n",
      "(270, 327, 185, 12)\n",
      "Time taken: 519.2818676700117\n",
      "Generating class 9 set from model 19tensor([5017, 2962])\n",
      "(279, 293, 205, 21)\n",
      "Time taken: 519.8846754789993\n",
      "Generating class 10 set from model 19tensor([5003, 1781])\n",
      "(170, 399, 105, 5)\n",
      "Time taken: 492.35460632201284\n",
      "Generating class 11 set from model 19tensor([5010, 1332])\n",
      "(113, 449, 69, 4)\n",
      "Time taken: 489.27266010901076\n",
      "Generating class 12 set from model 19tensor([5067, 2394])\n",
      "(215, 371, 135, 26)\n",
      "Time taken: 505.8198121319874\n",
      "Generating class 13 set from model 19tensor([5028, 2342])\n",
      "(218, 380, 126, 13)\n",
      "Time taken: 506.5911254690145\n",
      "Generating class 14 set from model 19tensor([5000, 1891])\n",
      "(174, 410, 85, 21)\n",
      "Time taken: 497.76193306598\n",
      "Generating class 15 set from model 19tensor([5053, 1923])\n",
      "(168, 408, 107, 15)\n",
      "Time taken: 501.0197050110146\n",
      "Generating class 16 set from model 19tensor([4947, 2156])\n",
      "(211, 361, 119, 20)\n",
      "Time taken: 500.0714062299812\n",
      "Generating class 17 set from model 19tensor([4990, 3505])\n",
      "(337, 269, 215, 29)\n",
      "Time taken: 530.1242765299976\n",
      "Generating class 18 set from model 19tensor([5005, 1810])\n",
      "(165, 408, 86, 23)\n",
      "Time taken: 498.505139668996\n",
      "Generating class 19 set from model 19tensor([4962, 2033])\n",
      "(177, 395, 99, 29)\n",
      "Time taken: 505.7474564209988\n",
      "Generating class 20 set from model 19tensor([5001, 3939])\n",
      "(368, 197, 312, 17)\n",
      "Time taken: 555.3309846259945\n",
      "Generating class 21 set from model 19tensor([4955, 3651])\n",
      "(310, 283, 236, 32)\n",
      "Time taken: 544.9108726099948\n",
      "Generating class 22 set from model 19tensor([4932, 1822])\n",
      "(163, 382, 110, 21)\n",
      "Time taken: 505.0587922029954\n",
      "Generating class 23 set from model 19tensor([4972, 3305])\n",
      "(316, 258, 231, 23)\n",
      "Time taken: 531.3001230039808\n",
      "Generating class 24 set from model 19tensor([5035, 3292])\n",
      "(294, 275, 215, 49)\n",
      "Time taken: 532.8826902819856\n",
      "Generating class 25 set from model 19tensor([4997, 1775])\n",
      "(182, 395, 92, 9)\n",
      "Time taken: 500.06249563998426\n",
      "Generating class 26 set from model 19tensor([5053, 1698])\n",
      "(142, 435, 82, 17)\n",
      "Time taken: 502.98666534700897\n",
      "Generating class 27 set from model 19tensor([4986, 2224])\n",
      "(177, 404, 90, 50)\n",
      "Time taken: 513.4479241819936\n",
      "Generating class 28 set from model 19tensor([4990, 3421])\n",
      "(298, 270, 220, 54)\n",
      "Time taken: 533.2397546130233\n",
      "Generating class 29 set from model 19tensor([5011, 2236])\n",
      "(212, 369, 137, 7)\n",
      "Time taken: 509.0110361019906\n",
      "Generating class 30 set from model 19tensor([5044, 2600])\n",
      "(223, 337, 177, 28)\n",
      "Time taken: 515.2795883259969\n",
      "Generating class 31 set from model 19tensor([4992, 3036])\n",
      "(283, 325, 185, 10)\n",
      "Time taken: 525.6039321770077\n",
      "Generating class 32 set from model 19tensor([4972, 1973])\n",
      "(171, 405, 93, 26)\n",
      "Time taken: 505.5025787180057\n",
      "Generating class 33 set from model 19tensor([5034, 2707])\n",
      "(249, 314, 187, 25)\n",
      "Time taken: 520.53285860701\n",
      "Generating class 34 set from model 19tensor([4970, 2001])\n",
      "(164, 417, 100, 17)\n",
      "Time taken: 509.57405270499294\n",
      "Generating class 35 set from model 19tensor([4996, 1678])\n",
      "(150, 428, 73, 17)\n",
      "Time taken: 500.54662243899656\n",
      "Generating class 36 set from model 19tensor([4910, 2860])\n",
      "(275, 316, 173, 13)\n",
      "Time taken: 517.142823422997\n",
      "Generating class 37 set from model 19tensor([4964, 2210])\n",
      "(168, 402, 118, 30)\n",
      "Time taken: 511.39443268300965\n",
      "Generating class 38 set from model 19tensor([4970, 1889])\n",
      "(161, 403, 91, 31)\n",
      "Time taken: 501.65674293998745\n",
      "Generating class 39 set from model 19tensor([5023, 2955])\n",
      "(273, 337, 174, 14)\n",
      "Time taken: 524.9909486690012\n",
      "Generating class 40 set from model 19tensor([5055, 2166])\n",
      "(200, 370, 143, 10)\n",
      "Time taken: 509.8700242120249\n",
      "Generating class 41 set from model 19tensor([5032, 3371])\n",
      "(333, 255, 242, 11)\n",
      "Time taken: 533.2915405700041\n",
      "Generating class 42 set from model 19tensor([4957, 2148])\n",
      "(191, 392, 108, 20)\n",
      "Time taken: 511.47353601898067\n",
      "Generating class 43 set from model 19tensor([4986, 2783])\n",
      "(249, 335, 161, 32)\n",
      "Time taken: 519.2949309140095\n",
      "Generating class 44 set from model 17"
     ]
    }
   ],
   "source": [
    "# Create 100 attack model training sets, one for each class\n",
    "# These will be used to train 100 attack models, as per Shokri et al.\n",
    "\n",
    "sm = nn.Softmax()\n",
    "for c in range(100):\n",
    "  starttime = time.process_time()\n",
    "  attack_x = []\n",
    "  attack_y = []\n",
    "  # Generate attack training set for current class\n",
    "  for i, shadow_model_set in enumerate(shadow_models):\n",
    "    print(f\"\\rGenerating class {c} set from model {i}\", end=\"\")\n",
    "    shadow_model = shadow_model_set[0]\n",
    "    in_loader = torch.utils.data.DataLoader(shadow_datasets[i][0], batch_size=1)\n",
    "    for data, target in in_loader:\n",
    "      if target == c:\n",
    "        pred = shadow_model(data).view(100)\n",
    "        if torch.argmax(pred).item() == c:\n",
    "            attack_x.append(sm(pred))\n",
    "            attack_y.append(1)\n",
    "    out_loader = torch.utils.data.DataLoader(shadow_datasets[i][1], batch_size=1)\n",
    "    for data, target in out_loader:\n",
    "      if target == c:\n",
    "        pred = shadow_model(data).view(100)\n",
    "        attack_x.append(sm(pred))\n",
    "        attack_y.append(0)\n",
    "              \n",
    "  # Save datasets\n",
    "  tensor_x = torch.stack(attack_x)\n",
    "  tensor_y = torch.Tensor(attack_y)\n",
    "  xpath = f\"infattack/resnet_attack_x_{c}.pt\"\n",
    "  ypath = f\"infattack/resnet_attack_y_{c}.pt\"\n",
    "  torch.save(tensor_x, xpath)\n",
    "  torch.save(tensor_y, ypath)\n",
    "  tensor_x = torch.load(f\"infattack/resnet_attack_x_{c}.pt\")\n",
    "  tensor_y = torch.load(f\"infattack/resnet_attack_y_{c}.pt\")\n",
    "  print(torch.unique(tensor_y, return_counts=True)[1])\n",
    "  \n",
    "  # Create test and train dataloaders for attack dataset\n",
    "  attack_datasets = []\n",
    "  attack_datasets.append(torch.utils.data.TensorDataset(tensor_x, tensor_y))\n",
    "  attack_train, attack_test = torch.utils.data.random_split(\n",
    "    attack_datasets[0], [int(0.9*len(attack_datasets[0])), \n",
    "    len(attack_datasets[0]) - int(0.9*len(attack_datasets[0]))])\n",
    "  attackloader = torch.utils.data.DataLoader(attack_train, batch_size=batch_size, shuffle=True)\n",
    "  attacktester = torch.utils.data.DataLoader(attack_test, batch_size=batch_size, shuffle=True)\n",
    "  \n",
    "  # Create and train an attack model\n",
    "  attack_model, attack_optimizer = attack_model_fn()\n",
    "  for epoch in range(10):\n",
    "    trainattacker(attack_model, attack_optimizer, epoch, attackloader, printable=False)\n",
    "  print(fulltestattacker(attack_model, attacktester, dname=f\"Class {c}\"))\n",
    "  \n",
    "  # Save attack model\n",
    "  path = F\"infattack/resnet_attack_model_{c}.pt\"\n",
    "  torch.save({\n",
    "        'model_state_dict': attack_model.state_dict(),\n",
    "        }, path)\n",
    "  print(f\"Time taken: {time.process_time() - starttime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = targetclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant datasets and create test dataloader\n",
    "tensor_x = torch.load(f\"infattack/resnet_attack_x_{c}.pt\")\n",
    "tensor_y = torch.load(f\"infattack/resnet_attack_y_{c}.pt\")\n",
    "attack_datasets = []\n",
    "attack_datasets.append(torch.utils.data.TensorDataset(tensor_x, tensor_y))\n",
    "attacktester = torch.utils.data.DataLoader(attack_datasets[0], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attack_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8e41c5eeaffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mF\"infattack/resnet_attack_model_{c}.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mattack_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'attack_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load relevant attack model\n",
    "path = F\"infattack/resnet_attack_model_{c}.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "attack_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1316, 4409, 601, 16)\n"
     ]
    }
   ],
   "source": [
    "print(fulltestattacker(attack_model, attacktester, dname=f\"Class {c}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TargetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual target model to attack, trained in the same\n",
    "# way as the shadow models\n",
    "\n",
    "targetmodel, targetoptim = target_model_fn()\n",
    "trainingepochs = 10\n",
    "log_interval = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data, out_data = torch.utils.data.random_split(traindata, [int(len(traindata)/2), int(len(traindata)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [ 24576]\tLoss: 3.238380Time taken: 1007.0192443899999\n",
      "Epoch: 2 [ 24576]\tLoss: 3.808367Time taken: 996.1704727689998\n",
      "Epoch: 3 [ 24576]\tLoss: 4.190974Time taken: 997.652983787\n",
      "Epoch: 4 [ 24576]\tLoss: 3.177373Time taken: 999.523929\n",
      "Epoch: 5 [ 24576]\tLoss: 2.540803Time taken: 1002.6273063850003\n",
      "Epoch: 6 [ 24576]\tLoss: 4.309068Time taken: 1004.036909247\n",
      "Epoch: 7 [ 24576]\tLoss: 2.032048Time taken: 1013.511792663\n",
      "Epoch: 8 [ 24576]\tLoss: 3.616669Time taken: 1015.2767908240003\n",
      "Epoch: 9 [ 24576]\tLoss: 3.130907Time taken: 1017.9769915679999\n",
      "Epoch: 10 [ 24576]\tLoss: 3.527303Time taken: 1019.2529550299987\n",
      "All data: Mean loss: 0.3944, Accuracy: 7371/10000 (74%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7371"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = []\n",
    "in_loader = torch.utils.data.DataLoader(in_data, batch_size=batch_size, shuffle=True)\n",
    "out_loader = torch.utils.data.DataLoader(out_data, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(1, trainingepochs+1):\n",
    "    print(f\"\\rEpoch {epoch}  \"  , end=\"\")\n",
    "    starttime = time.process_time()\n",
    "    steps.append(selectivetrain(targetmodel, targetoptim, epoch, in_loader, returnable=True))\n",
    "    print(f\"Time taken: {time.process_time() - starttime}\")\n",
    "test(targetmodel, testloader, dname=\"All data\", printable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = F\"infattack/cnn_target_trained.pt\"\n",
    "torch.save({\n",
    "            'model_state_dict': targetmodel.state_dict(),\n",
    "            'optimizer_state_dict': targetoptim.state_dict(),\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"infattack/cnn_batches.pkl\", \"wb\")\n",
    "pickle.dump(steps, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches effected: 10/31250 = 0.032%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Batches effected: {len(steps)}/{len(in_loader)*trainingepochs} = {100*len(steps)/(len(in_loader)*trainingepochs)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_loader = torch.utils.data.DataLoader(in_data, batch_size=1, shuffle=False)\n",
    "out_loader = torch.utils.data.DataLoader(out_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating class 11 set from target model"
     ]
    }
   ],
   "source": [
    "# Create 100 attack model training sets, one for each class\n",
    "# These will be used to train 100 attack models, as per Shokri et al.\n",
    "# Currently configured to only produce for the target class\n",
    "attack_datasets = []\n",
    "sm = nn.Softmax()\n",
    "for c in range(targetclass, targetclass+1):\n",
    "    targetmodel.eval()\n",
    "    attackdata_x = []\n",
    "    attackdata_y = []\n",
    "    count = 0\n",
    "    print(f\"\\rGenerating class {c} set from target model\", end=\"\")\n",
    "    for data, target in in_loader:\n",
    "      if target == c:\n",
    "        pred = targetmodel(data).view(100)\n",
    "        if torch.argmax(pred).item() == c:\n",
    "            attackdata_x.append(data)\n",
    "            attackdata_y.append(1)\n",
    "            count += 1\n",
    "    for data, target in out_loader:\n",
    "      if target == c:\n",
    "        attackdata_x.append(data)\n",
    "        attackdata_y.append(0)\n",
    "        count += 1\n",
    "    attack_tensor_x = torch.stack(attackdata_x)\n",
    "    attack_tensor_y = torch.Tensor(attackdata_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_data = torch.utils.data.TensorDataset(attack_tensor_x, attack_tensor_y)\n",
    "atk_loader = torch.utils.data.DataLoader(atk_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testtargetmodel():\n",
    "    attack_datasets = []\n",
    "    sm = nn.Softmax()\n",
    "    for c in range(targetclass, targetclass+1):\n",
    "        targetmodel.eval()\n",
    "        attack_x = []\n",
    "        attack_y = []\n",
    "        for data, target in atk_loader:\n",
    "            data = data.reshape(1,3,32,32)\n",
    "            pred = targetmodel(data).view(100)\n",
    "            attack_x.append(sm(pred))\n",
    "            attack_y.append(target)\n",
    "        tensor_x = torch.stack(attack_x)\n",
    "        tensor_y = torch.Tensor(attack_y)\n",
    "        path = F\"infattack/resnet_attack_model_{c}.pt\"\n",
    "        checkpoint = torch.load(path)\n",
    "        attack_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        attack_datasets = []\n",
    "        attack_datasets.append(torch.utils.data.TensorDataset(tensor_x, tensor_y))\n",
    "        attacktester = torch.utils.data.DataLoader(attack_datasets[0], batch_size=batch_size, shuffle=True)\n",
    "        tp, tn, fp, fn = fulltestattacker(attack_model, attacktester, dname=f\"\\rclass {c}\")\n",
    "        recall = tp / (tp + fn)\n",
    "        print(f\"\\trecall: {recall}\")\n",
    "        missrate = fn / (fn + tp)\n",
    "#         print(f\"\\tmissrate: {missrate}\")\n",
    "        return recall, missrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model, _ = attack_model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = F\"infattack/cnn_target_trained.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "targetmodel.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trecall: 0.9523809523809523\n",
      "\trecall: 0.12698412698412698\n",
      "Epoch 0  \trecall: 0.015873015873015872\n",
      "Epoch 1  \trecall: 0.0\n",
      "Epoch 2  \trecall: 0.0\n",
      "Epoch 3  \trecall: 0.0\n",
      "Epoch 4  \trecall: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test amnesiac unlearning by testing membership inference\n",
    "# attack results on unprotected model, model after amnesiac\n",
    "# step, and after each epoch of further training\n",
    "\n",
    "recall = []\n",
    "missrate = []\n",
    "r, m = testtargetmodel()\n",
    "recall.append(r)\n",
    "missrate.append(m)\n",
    "for step in steps:\n",
    "    const = 1\n",
    "    with torch.no_grad():\n",
    "        state = targetmodel.state_dict()\n",
    "        for param_tensor in state:\n",
    "            if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "              state[param_tensor] = state[param_tensor] - const*step[param_tensor]\n",
    "    targetmodel.load_state_dict(state)\n",
    "r, m = testtargetmodel()\n",
    "recall.append(r)\n",
    "missrate.append(m)\n",
    "for epoch in range(5):\n",
    "    print(f\"\\rEpoch {epoch}  \"  , end=\"\")\n",
    "    starttime = time.process_time()\n",
    "    r, m = train2(targetmodel, targetoptim, epoch, nontarget_train_loader, printable=False)\n",
    "    recall = recall + r\n",
    "#     print(f\"Time taken: {time.process_time() - starttime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9523809523809523, 0.12698412698412698, 0.015873015873015872, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn mia datagen.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.4-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0024b395aaec4983a874c9c3f92af84e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c20e162490c433e83a3b82ac973a826",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d901d93b5f09484ea6c8ea6dfb9d8bcc",
      "value": 1
     }
    },
    "01824765ed9e4234adca0786222543ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c20e162490c433e83a3b82ac973a826": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c3251b4e7144cd3a5a92c2d8c00854a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0024b395aaec4983a874c9c3f92af84e",
       "IPY_MODEL_7d3fb8e3c37e46ebaaa1967e787e6a4e"
      ],
      "layout": "IPY_MODEL_01824765ed9e4234adca0786222543ff"
     }
    },
    "64d5070ef1d149a3af18a3b1fa0e3989": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d3fb8e3c37e46ebaaa1967e787e6a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64d5070ef1d149a3af18a3b1fa0e3989",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aede667a8fa3464080eb6e029b271bcf",
      "value": " 169009152/? [00:30&lt;00:00, 16549487.85it/s]"
     }
    },
    "aede667a8fa3464080eb6e029b271bcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d901d93b5f09484ea6c8ea6dfb9d8bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}